{
  "components": {
    "comp-ingest-data": {
      "executorLabel": "exec-ingest-data",
      "inputDefinitions": {
        "artifacts": {
          "input_files": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            },
            "description": "Input dataset containing documents"
          }
        },
        "parameters": {
          "data_store_id": {
            "description": "ID of target datastore",
            "parameterType": "STRING"
          },
          "data_store_region": {
            "description": "Region for Vertex AI Search",
            "parameterType": "STRING"
          },
          "embedding_column": {
            "defaultValue": "embedding",
            "description": "Name of embedding column in schema",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "embedding_dimension": {
            "defaultValue": 768.0,
            "isOptional": true,
            "parameterType": "NUMBER_INTEGER"
          },
          "project_id": {
            "description": "Google Cloud project ID",
            "parameterType": "STRING"
          }
        }
      }
    },
    "comp-process-data": {
      "executorLabel": "exec-process-data",
      "inputDefinitions": {
        "parameters": {
          "chunk_overlap": {
            "defaultValue": 20.0,
            "description": "Overlap between chunks",
            "isOptional": true,
            "parameterType": "NUMBER_INTEGER"
          },
          "chunk_size": {
            "defaultValue": 1500.0,
            "description": "Size of text chunks",
            "isOptional": true,
            "parameterType": "NUMBER_INTEGER"
          },
          "deduped_table": {
            "defaultValue": "questions_embeddings",
            "description": "Table for storing deduplicated results",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "destination_dataset": {
            "defaultValue": "stackoverflow_data",
            "description": "BigQuery dataset for storing results",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "destination_table": {
            "defaultValue": "incremental_questions_embeddings",
            "description": "Table for storing incremental results",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "embedding_column": {
            "defaultValue": "embedding",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "is_incremental": {
            "defaultValue": true,
            "description": "Whether to process only recent data",
            "isOptional": true,
            "parameterType": "BOOLEAN"
          },
          "location": {
            "defaultValue": "us-central1",
            "description": "BigQuery location",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "look_back_days": {
            "defaultValue": 1.0,
            "description": "Number of days to look back for incremental processing",
            "isOptional": true,
            "parameterType": "NUMBER_INTEGER"
          },
          "project_id": {
            "parameterType": "STRING"
          },
          "schedule_time": {
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "output_files": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          }
        }
      }
    }
  },
  "deploymentSpec": {
    "executors": {
      "exec-ingest-data": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "ingest_data"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef ingest_data(\n    project_id: str,\n    data_store_region: str,\n    input_files: Input[Dataset],\n    data_store_id: str,\n    embedding_dimension: int = 768,\n    embedding_column: str = \"embedding\",\n) -> None:\n    \"\"\"Process and ingest documents into Vertex AI Search datastore.\n\n    Args:\n        project_id: Google Cloud project ID\n        data_store_region: Region for Vertex AI Search\n        input_files: Input dataset containing documents\n        data_store_id: ID of target datastore\n        embedding_column: Name of embedding column in schema\n    \"\"\"\n    import json\n    import logging\n    import time\n\n    from google.api_core.client_options import ClientOptions\n    from google.cloud import discoveryengine\n\n    def update_schema_as_json(\n        original_schema: str,\n        embedding_dimension: int,\n        field_name: str | None = None,\n    ) -> str:\n        \"\"\"Update datastore schema JSON to include embedding field.\n\n        Args:\n            original_schema: Original schema JSON string\n            field_name: Name of embedding field to add\n\n        Returns:\n            Updated schema JSON string\n        \"\"\"\n        original_schema_dict = json.loads(original_schema)\n\n        if original_schema_dict.get(\"properties\") is None:\n            original_schema_dict[\"properties\"] = {}\n\n        if field_name:\n            field_schema = {\n                \"type\": \"array\",\n                \"keyPropertyMapping\": \"embedding_vector\",\n                \"dimension\": embedding_dimension,\n                \"items\": {\"type\": \"number\"},\n            }\n            original_schema_dict[\"properties\"][field_name] = field_schema\n\n        return json.dumps(original_schema_dict)\n\n    def update_data_store_schema(\n        project_id: str,\n        location: str,\n        data_store_id: str,\n        field_name: str | None = None,\n        client_options: ClientOptions | None = None,\n    ) -> None:\n        \"\"\"Update datastore schema to include embedding field.\n\n        Args:\n            project_id: Google Cloud project ID\n            location: Google Cloud location\n            data_store_id: Target datastore ID\n            embedding_column: Name of embedding column\n            client_options: Client options for API\n        \"\"\"\n        schema_client = discoveryengine.SchemaServiceClient(\n            client_options=client_options\n        )\n        collection = \"default_collection\"\n\n        name = f\"projects/{project_id}/locations/{location}/collections/{collection}/dataStores/{data_store_id}/schemas/default_schema\"\n\n        schema = schema_client.get_schema(\n            request=discoveryengine.GetSchemaRequest(name=name)\n        )\n        new_schema_json = update_schema_as_json(\n            original_schema=schema.json_schema,\n            embedding_dimension=embedding_dimension,\n            field_name=field_name,\n        )\n        new_schema = discoveryengine.Schema(json_schema=new_schema_json, name=name)\n\n        operation = schema_client.update_schema(\n            request=discoveryengine.UpdateSchemaRequest(\n                schema=new_schema, allow_missing=True\n            ),\n            timeout=1800,\n        )\n        logging.info(f\"Waiting for schema update operation: {operation.operation.name}\")\n        operation.result()\n\n    def add_data_in_store(\n        project_id: str,\n        location: str,\n        data_store_id: str,\n        input_files_uri: str,\n        client_options: ClientOptions | None = None,\n    ) -> None:\n        \"\"\"Import documents into datastore.\n\n        Args:\n            project_id: Google Cloud project ID\n            location: Google Cloud location\n            data_store_id: Target datastore ID\n            input_files_uri: URI of input files\n            client_options: Client options for API\n        \"\"\"\n        client = discoveryengine.DocumentServiceClient(client_options=client_options)\n\n        parent = client.branch_path(\n            project=project_id,\n            location=location,\n            data_store=data_store_id,\n            branch=\"default_branch\",\n        )\n\n        request = discoveryengine.ImportDocumentsRequest(\n            parent=parent,\n            gcs_source=discoveryengine.GcsSource(\n                input_uris=[input_files_uri],\n                data_schema=\"document\",\n            ),\n            reconciliation_mode=discoveryengine.ImportDocumentsRequest.ReconciliationMode.FULL,\n        )\n\n        operation = client.import_documents(request=request)\n        logging.info(f\"Waiting for import operation: {operation.operation.name}\")\n        operation.result()\n\n    client_options = ClientOptions(\n        api_endpoint=f\"{data_store_region}-discoveryengine.googleapis.com\"\n    )\n\n    logging.info(\"Updating data store schema...\")\n    update_data_store_schema(\n        project_id=project_id,\n        location=data_store_region,\n        data_store_id=data_store_id,\n        field_name=embedding_column,\n        client_options=client_options,\n    )\n    logging.info(\"Schema updated successfully\")\n\n    logging.info(\"Importing data into store...\")\n    add_data_in_store(\n        project_id=project_id,\n        location=data_store_region,\n        data_store_id=data_store_id,\n        client_options=client_options,\n        input_files_uri=input_files.uri,\n    )\n    logging.info(\"Data import completed\")\n    logging.info(\n        \"Sleeping for 3 minutes to allow Vertex AI Search to properly index the data...\"\n    )\n    time.sleep(180)  # Sleep for 180 seconds (3 minutes)\n    logging.info(\"Sleep completed. Data indexing should now be complete.\")\n\n"
          ],
          "image": "us-docker.pkg.dev/production-ai-template/starter-pack/data_processing:0.2"
        }
      },
      "exec-process-data": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "process_data"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef process_data(\n    project_id: str,\n    schedule_time: str,\n    output_files: Output[Dataset],\n    is_incremental: bool = True,\n    look_back_days: int = 1,\n    chunk_size: int = 1500,\n    chunk_overlap: int = 20,\n    destination_dataset: str = \"stackoverflow_data\",\n    destination_table: str = \"incremental_questions_embeddings\",\n    deduped_table: str = \"questions_embeddings\",\n    location: str = \"us-central1\",\n    embedding_column: str = \"embedding\",\n) -> None:\n    \"\"\"Process StackOverflow questions and answers by:\n    1. Fetching data from BigQuery\n    2. Converting HTML to markdown\n    3. Splitting text into chunks\n    4. Generating embeddings\n    5. Storing results in BigQuery\n    6. Exporting to JSONL\n\n    Args:\n        output_files: Output dataset path\n        is_incremental: Whether to process only recent data\n        look_back_days: Number of days to look back for incremental processing\n        chunk_size: Size of text chunks\n        chunk_overlap: Overlap between chunks\n        destination_dataset: BigQuery dataset for storing results\n        destination_table: Table for storing incremental results\n        deduped_table: Table for storing deduplicated results\n        location: BigQuery location\n    \"\"\"\n    import logging\n    from datetime import datetime, timedelta\n\n    import backoff\n    import bigframes.ml.llm as llm\n    import bigframes.pandas as bpd\n    import google.api_core.exceptions\n    import swifter\n    from google.cloud import bigquery\n    from langchain.text_splitter import RecursiveCharacterTextSplitter\n    from markdownify import markdownify\n\n    # Initialize logging\n    logging.basicConfig(level=logging.INFO)\n    logging.info(f\"Using {swifter} for apply operations.\")\n\n    # Initialize clients\n    logging.info(\"Initializing clients...\")\n    bq_client = bigquery.Client(project=project_id, location=location)\n    bpd.options.bigquery.project = project_id\n    bpd.options.bigquery.location = location\n    logging.info(\"Clients initialized.\")\n\n    # Set date range for data fetch\n    schedule_time_dt: datetime = datetime.fromisoformat(\n        schedule_time.replace(\"Z\", \"+00:00\")\n    )\n    if schedule_time_dt.year == 1970:\n        logging.warning(\n            \"Pipeline schedule not set. Setting schedule_time to current date.\"\n        )\n        schedule_time_dt = datetime.now()\n\n    # Note: The following line sets the schedule time 5 years back to allow sample data to be present.\n    # For your use case, please comment out the following line to use the actual schedule time.\n    schedule_time_dt = schedule_time_dt - timedelta(days=5 * 365)\n\n    START_DATE: datetime = schedule_time_dt - timedelta(\n        days=look_back_days\n    )  # Start date for data processing window\n    END_DATE: datetime = schedule_time_dt  # End date for data processing window\n\n    logging.info(f\"Date range set: START_DATE={START_DATE}, END_DATE={END_DATE}\")\n\n    def fetch_stackoverflow_data(\n        dataset_suffix: str, start_date: str, end_date: str\n    ) -> bpd.DataFrame:\n        \"\"\"Fetch StackOverflow data from BigQuery.\"\"\"\n        query = f\"\"\"\n            SELECT\n                creation_date,\n                last_edit_date,\n                question_id,\n                question_title,\n                question_body AS question_text,\n                answers\n            FROM `production-ai-template.stackoverflow_qa_{dataset_suffix}.stackoverflow_python_questions_and_answers`\n            WHERE TRUE\n                {f'AND TIMESTAMP_TRUNC(creation_date, DAY) BETWEEN TIMESTAMP(\"{start_date}\") AND TIMESTAMP(\"{end_date}\")' if is_incremental else \"\"}\n        \"\"\"\n        logging.info(\"Fetching StackOverflow data from BigQuery...\")\n        return bpd.read_gbq(query)\n\n    def convert_html_to_markdown(html: str) -> str:\n        \"\"\"Convert HTML into Markdown for easier parsing and rendering after LLM response.\"\"\"\n        return markdownify(html).strip()\n\n    def create_answers_markdown(answers: list) -> str:\n        \"\"\"Convert each answer's HTML to markdown and concatenate into a single markdown text.\"\"\"\n        answers_md = \"\"\n        for index, answer_record in enumerate(answers):\n            answers_md += (\n                f\"\\n\\n## Answer {index + 1}:\\n\"  # Answer number is H2 heading size\n            )\n            answers_md += convert_html_to_markdown(answer_record[\"body\"])\n        return answers_md\n\n    def create_table_if_not_exist(\n        df: bpd.DataFrame,\n        project_id: str,\n        dataset_id: str,\n        table_id: str,\n        partition_column: str,\n        location: str = location,\n    ) -> None:\n        \"\"\"Create BigQuery table with time partitioning if it doesn't exist.\"\"\"\n        table_schema = bq_client.get_table(df.head(0).to_gbq()).schema\n        table = bigquery.Table(\n            f\"{project_id}.{dataset_id}.{table_id}\", schema=table_schema\n        )\n        table.time_partitioning = bigquery.TimePartitioning(\n            type_=bigquery.TimePartitioningType.DAY, field=partition_column\n        )\n\n        dataset = bigquery.Dataset(f\"{project_id}.{dataset_id}\")\n        dataset.location = location\n        bq_client.create_dataset(dataset, exists_ok=True)\n        bq_client.create_table(table=table, exists_ok=True)\n\n    # Fetch and preprocess data\n    logging.info(\"Fetching and preprocessing data...\")\n    df = fetch_stackoverflow_data(\n        start_date=START_DATE.strftime(\"%Y-%m-%d\"),\n        end_date=END_DATE.strftime(\"%Y-%m-%d\"),\n        dataset_suffix=location.lower().replace(\"-\", \"_\"),\n    )\n    df = (\n        df.sort_values(\"last_edit_date\", ascending=False)\n        .drop_duplicates(\"question_id\")\n        .reset_index(drop=True)\n    )\n    logging.info(\"Data fetched and preprocessed.\")\n\n    # Convert content to markdown\n    logging.info(\"Converting content to markdown...\")\n\n    # Create markdown fields efficiently\n    df[\"question_title_md\"] = (\n        \"# \" + df[\"question_title\"] + \"\\n\"\n    )  # Title is H1 heading size\n    df[\"question_text_md\"] = (\n        df[\"question_text\"].to_pandas().swifter.apply(convert_html_to_markdown) + \"\\n\"\n    )\n    df[\"answers_md\"] = df[\"answers\"].to_pandas().swifter.apply(create_answers_markdown)\n\n    # Create a column containing the whole markdown text\n    df[\"full_text_md\"] = (\n        df[\"question_title_md\"] + df[\"question_text_md\"] + df[\"answers_md\"]\n    )\n    logging.info(\"Content converted to markdown.\")\n\n    # Keep only necessary columns\n    df = df[[\"last_edit_date\", \"question_id\", \"question_text\", \"full_text_md\"]]\n\n    # Split text into chunks\n    logging.info(\"Splitting text into chunks...\")\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap,\n        length_function=len,\n    )\n\n    df[\"text_chunk\"] = (\n        df[\"full_text_md\"]\n        .to_pandas()\n        .astype(object)\n        .swifter.apply(text_splitter.split_text)\n    )\n    logging.info(\"Text split into chunks.\")\n\n    # Create chunk IDs and explode chunks into rows\n    logging.info(\"Creating chunk IDs and exploding chunks into rows...\")\n    chunk_ids = [\n        str(idx) for text_chunk in df[\"text_chunk\"] for idx in range(len(text_chunk))\n    ]\n    df = df.explode(\"text_chunk\").reset_index(drop=True)\n    df[\"chunk_id\"] = df[\"question_id\"].astype(\"string\") + \"__\" + chunk_ids\n    logging.info(\"Chunk IDs created and chunks exploded.\")\n\n    # Generate embeddings\n    logging.info(\"Generating embeddings...\")\n\n    # The first invocation in a new project might fail due to permission propagation.\n    @backoff.on_exception(\n        backoff.expo, google.api_core.exceptions.InvalidArgument, max_tries=10\n    )\n    def create_embedder() -> llm.TextEmbeddingGenerator:\n        return llm.TextEmbeddingGenerator(model_name=\"text-embedding-005\")\n\n    embedder = create_embedder()\n\n    embeddings_df = embedder.predict(df[\"text_chunk\"])\n    logging.info(\"Embeddings generated.\")\n\n    df = df.assign(\n        embedding=embeddings_df[\"ml_generate_embedding_result\"],\n        embedding_statistics=embeddings_df[\"ml_generate_embedding_statistics\"],\n        embedding_status=embeddings_df[\"ml_generate_embedding_status\"],\n        creation_timestamp=datetime.now(),\n    )\n\n    # Store results in BigQuery\n    PARTITION_DATE_COLUMN = \"creation_timestamp\"\n\n    # Create and populate incremental table\n    logging.info(\"Creating and populating incremental table...\")\n    create_table_if_not_exist(\n        df=df,\n        project_id=project_id,\n        dataset_id=destination_dataset,\n        table_id=destination_table,\n        partition_column=PARTITION_DATE_COLUMN,\n    )\n\n    if_exists_mode = \"append\" if is_incremental else \"replace\"\n    df.to_gbq(\n        destination_table=f\"{destination_dataset}.{destination_table}\",\n        if_exists=if_exists_mode,\n    )\n    logging.info(\"Incremental table created and populated.\")\n\n    # Create deduplicated table\n    logging.info(\"Creating deduplicated table...\")\n    df_questions = bpd.read_gbq(\n        f\"{destination_dataset}.{destination_table}\", use_cache=False\n    )\n    max_date_df = (\n        df_questions.groupby(\"question_id\")[\"creation_timestamp\"].max().reset_index()\n    )\n    df_questions_dedup = max_date_df.merge(\n        df_questions, how=\"inner\", on=[\"question_id\", \"creation_timestamp\"]\n    )\n\n    create_table_if_not_exist(\n        df=df_questions_dedup,\n        project_id=project_id,\n        dataset_id=destination_dataset,\n        table_id=deduped_table,\n        partition_column=PARTITION_DATE_COLUMN,\n    )\n\n    df_questions_dedup.to_gbq(\n        destination_table=f\"{destination_dataset}.{deduped_table}\",\n        if_exists=\"replace\",\n    )\n    logging.info(\"Deduplicated table created and populated.\")\n\n    # Export to JSONL\n    logging.info(\"Exporting to JSONL...\")\n\n    export_query = f\"\"\"\n    SELECT\n        chunk_id as id,\n        TO_JSON_STRING(STRUCT(\n            chunk_id as id,\n            embedding as {embedding_column},\n            text_chunk as content,\n            question_id,\n            CAST(creation_timestamp AS STRING) as creation_timestamp,\n            CAST(last_edit_date AS STRING) as last_edit_date,\n            question_text,\n            full_text_md\n        )) as json_data\n    FROM\n        `{project_id}.{destination_dataset}.{deduped_table}`\n    WHERE\n        chunk_id IS NOT NULL\n        AND embedding IS NOT NULL\n    \"\"\"\n    export_df_id = bpd.read_gbq(export_query).to_gbq()\n\n    output_files.uri = output_files.uri + \"*.jsonl\"\n\n    job_config = bigquery.ExtractJobConfig()\n    job_config.destination_format = bigquery.DestinationFormat.NEWLINE_DELIMITED_JSON\n\n    extract_job = bq_client.extract_table(\n        export_df_id, output_files.uri, job_config=job_config\n    )\n    extract_job.result()\n    logging.info(\"Exported to JSONL.\")\n\n"
          ],
          "image": "us-docker.pkg.dev/production-ai-template/starter-pack/data_processing:0.2",
          "resources": {
            "cpuLimit": 1.0,
            "memoryLimit": 2.147483648,
            "resourceCpuLimit": "1",
            "resourceMemoryLimit": "2Gi"
          }
        }
      }
    }
  },
  "pipelineInfo": {
    "description": "A pipeline to run ingestion of new data into the datastore",
    "name": "pipeline"
  },
  "root": {
    "dag": {
      "tasks": {
        "ingest-data": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-ingest-data"
          },
          "dependentTasks": [
            "process-data"
          ],
          "inputs": {
            "artifacts": {
              "input_files": {
                "taskOutputArtifact": {
                  "outputArtifactKey": "output_files",
                  "producerTask": "process-data"
                }
              }
            },
            "parameters": {
              "data_store_id": {
                "componentInputParameter": "data_store_id"
              },
              "data_store_region": {
                "componentInputParameter": "data_store_region"
              },
              "embedding_column": {
                "runtimeValue": {
                  "constant": "embedding"
                }
              },
              "project_id": {
                "componentInputParameter": "project_id"
              }
            }
          },
          "retryPolicy": {
            "backoffDuration": "0s",
            "backoffFactor": 2.0,
            "backoffMaxDuration": "3600s",
            "maxRetryCount": 2
          },
          "taskInfo": {
            "name": "ingest-data"
          }
        },
        "process-data": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-process-data"
          },
          "inputs": {
            "parameters": {
              "chunk_overlap": {
                "componentInputParameter": "chunk_overlap"
              },
              "chunk_size": {
                "componentInputParameter": "chunk_size"
              },
              "deduped_table": {
                "componentInputParameter": "deduped_table"
              },
              "destination_dataset": {
                "componentInputParameter": "destination_dataset"
              },
              "destination_table": {
                "componentInputParameter": "destination_table"
              },
              "embedding_column": {
                "runtimeValue": {
                  "constant": "embedding"
                }
              },
              "is_incremental": {
                "componentInputParameter": "is_incremental"
              },
              "location": {
                "componentInputParameter": "location"
              },
              "look_back_days": {
                "componentInputParameter": "look_back_days"
              },
              "project_id": {
                "componentInputParameter": "project_id"
              },
              "schedule_time": {
                "runtimeValue": {
                  "constant": "{{$.pipeline_job_schedule_time_utc}}"
                }
              }
            }
          },
          "retryPolicy": {
            "backoffDuration": "0s",
            "backoffFactor": 2.0,
            "backoffMaxDuration": "3600s",
            "maxRetryCount": 2
          },
          "taskInfo": {
            "name": "process-data"
          }
        }
      }
    },
    "inputDefinitions": {
      "parameters": {
        "chunk_overlap": {
          "defaultValue": 20.0,
          "isOptional": true,
          "parameterType": "NUMBER_INTEGER"
        },
        "chunk_size": {
          "defaultValue": 1500.0,
          "isOptional": true,
          "parameterType": "NUMBER_INTEGER"
        },
        "data_store_id": {
          "defaultValue": "",
          "isOptional": true,
          "parameterType": "STRING"
        },
        "data_store_region": {
          "defaultValue": "",
          "isOptional": true,
          "parameterType": "STRING"
        },
        "deduped_table": {
          "defaultValue": "questions_embeddings",
          "isOptional": true,
          "parameterType": "STRING"
        },
        "destination_dataset": {
          "defaultValue": "my_awesome_agent_stackoverflow_data",
          "isOptional": true,
          "parameterType": "STRING"
        },
        "destination_table": {
          "defaultValue": "incremental_questions_embeddings",
          "isOptional": true,
          "parameterType": "STRING"
        },
        "is_incremental": {
          "defaultValue": true,
          "isOptional": true,
          "parameterType": "BOOLEAN"
        },
        "location": {
          "parameterType": "STRING"
        },
        "look_back_days": {
          "defaultValue": 1.0,
          "isOptional": true,
          "parameterType": "NUMBER_INTEGER"
        },
        "project_id": {
          "parameterType": "STRING"
        }
      }
    }
  },
  "schemaVersion": "2.1.0",
  "sdkVersion": "kfp-2.13.0"
}